{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Limesurvey ETL","text":"<p>This is the documentation of the Limesurvey ETL project. The goal is to provide an ETLaaS (ETL as a Service) application to process data from Limesurvey.</p>"},{"location":"#what-is-this-project-about-the-problem-goal","title":"What is this project about? - The Problem &amp; Goal","text":"<p>To become more data driven in their work, non-profits need to collect data. As an open source and privacy-aware software, Limesurvey is a good choice, especially for German NPOs.</p> <p>CFE uses Limesurvey to collect anti-discrimination and inclusion data (also known as equality data). Right now, data extraction from Limesurvey and initial data cleaning is done with a quite hacky R script with 1500 lines.</p> <p>The goal of this  project track is to design a pipeline to</p> <ol> <li>extract data from Limesurvey from its database via SSH tunnel</li> <li>perform necessary transformations to clean the raw data such as flagging speeders, creating variables for GDPR consent etc. and log the number of affected respondents for each step.</li> <li>load the cleaned data into a Postgres database</li> </ol> <p>The goal is that the resulting code is not specific to CFE data, but can be used by CFE and other NPOs working with Limesurvey in a \u201cplug-and-play\u201d way.</p> <p>(from the Calls for Applications)</p>"},{"location":"#project-organization","title":"Project Organization","text":"<p>TO BE DISCUSSED:</p> <pre><code>\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md           &lt;- The top-level README for developers using this project.\n\u2502\n\u251c\u2500\u2500 docs               &lt;- Markdown files used my [mkdocs](https://www.mkdocs.org/) to generate documentation.\n\u2502\n\u251c\u2500\u2500 requirements.txt   &lt;- The requirements file used in the docker-compose.yml\n\u2502\n\u251c\u2500\u2500 limesurvey_etl     &lt;- Source code for use in this project.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 config         &lt;- Contains the pydantic models for configuring ETL jobs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 extract_config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 transform_config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 load_config\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 connectors         &lt;- Contains the code used to connect to external systems\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 extract            &lt;- Contains the code used to extract data\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 transform          &lt;- Contains the code used to transform data\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 load               &lt;- Contains the code used to load data to target systems\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 etl_pipeline.py               &lt;- Contains code used to define an ETL pipeline\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 transformation_pipline.py &lt;- Contains code defining a transformation pipeline\n\u2502\n\u2514\u2500\u2500 poetry.\n</code></pre>"},{"location":"#access-data-in-limesurvey-playground-server","title":"Access Data in Limesurvey Playground Server","text":"<p>Yopass Link</p> <p>To access the data from the self-hosted Limesurvey Server and its MariaDB, you need to:</p> <ol> <li>connect to the virtual machine/server that Limesurvey is running on. This you do using SSH.</li> <li>then you can connect to the SQL database where the data is stored</li> </ol> <p>Interesting tables to take a look at first are probably:</p> <ul> <li>'lime_questions'</li> <li>'lime_question_attributes'</li> <li>'lime_survey_916481'</li> <li>'lime_survey_916481_timings'</li> <li>'lime_labels'</li> <li>'lime_answers'</li> </ul>"},{"location":"#connect-with-your-db-tool-of-choice-eg-beekeeper-studio","title":"Connect with your DB-Tool of Choice (e.g. Beekeeper Studio)","text":"<p>Enter the information provided in the Yopass Textfile and save the connection.</p>"},{"location":"#access-cleaned-data-in-coolify-postgres-database","title":"Access Cleaned Data in Coolify Postgres Database","text":"<p>Yopass Link</p> <p>This project team will try to create the pipeline that leads to a \u201ccleaned data\u201d structure in a Postgres. This means that data is already Q&amp;A'ed, cleaned and restructured. For reference to the end state, you can connect to the Coolify Postgres DB which contains the current state of CFE's cleaned data.</p> <p>Follow the instructions above, but instead use the credentials for the COOLIFY_PG database.</p> <p>Copy the content from the decrypted secret link. It should look something like this:</p> <pre><code># logins for Coolify Postgres DB\nCOOLIFY_PG_NAME='your-postgres-default-db-name'\nCOOLIFY_PG_HOST='your-postgres-url'\nCOOLIFY_PG_PORT='9001'\nCOOLIFY_PG_USER='your-postgres-user-name'\nCOOLIFY_PG_PASSWORD='your-postgres-pw'\n</code></pre>"},{"location":"#access-empty-coolify-mariadb-database","title":"Access Empty Coolify MariaDB Database","text":"<p>Yopass Link</p> <p>Follow the instructions above, but instead use the credentials for the COOLIFY_PG database. Copy the content from the decrypted secret link. It should look something like this:</p> <pre><code># logins for Coolify Postgres DB\nCOOLIFY_MARIA_NAME='your-maria-default-db-name'\nCOOLIFY_MARIA_HOST='your-maria-url'\nCOOLIFY_MARIA_PORT='9000'\nCOOLIFY_MARIA_USER='your-maria-user-name'\nCOOLIFY_MARIA_PASSWORD='your-maria-user-pw'\n</code></pre>"},{"location":"#access-coolify-playground-postgres-for-development","title":"Access Coolify Playground Postgres for Development","text":"<p>let Lisa know if this is needed</p>"},{"location":"#limitations","title":"Limitations","text":"<ul> <li>This project is still work-in-progress. Please report any bugs you may encounter.</li> <li>We couldn't collect any user feedback so far and the project structure solely relies on our own assumptions re. how to offer an ETLaaS (ETL as a Service) platform for Limesurvey data.</li> <li>Adding proper unit and integration tests is an open todo. There might be uncaught bugs in this application.</li> </ul>"},{"location":"#partner-organization","title":"Partner organization","text":"<p>This project was conducted in collaboration with the Vielfalt entscheidet project of Citizens For Europe gUG.</p>"},{"location":"installation/","title":"Getting Started","text":"<p>The following documentation describes the installation process that will get you up and running to implement your own ETL jobs.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<p>Install Docker and Docker Compose.</p> <p>If you want to contribute to this project or run an ETL pipeline locally without Airflow, you need to install the required dependencies.</p> <ul> <li><code>Python</code>&gt;=3.10.0</li> <li><code>poetry</code>&gt;=1.6.1</li> </ul> <p>Once you installed the dependencies, <code>cd</code> into the project's root directory in your terminal and run <code>poetry install --with dev</code> to install the required python dependencies into a virtual environment. To activate the virtual environment, run <code>poetry shell</code>.</p>"},{"location":"installation/#running-etl-pipelines","title":"Running ETL pipelines","text":"<p>You can run your ETL pipelines either locally with Python or using Airflow.</p>"},{"location":"installation/#1-running-an-etl-pipeline-locally","title":"1. Running an ETL pipeline locally","text":"<p>To run an ETL pipeline locally, you must set a number of environment variables.</p> <p>Create a <code>.env</code> file and store it at the project repository's root directory. Open the <code>.env</code> file you just created in your IDE or text editor and append the following three blocks of environment variables to it:</p> <ul> <li>Limesurvey DB related variables: These variables are required for the platform to connect with the Limesurvey database. You must provide the variables for establishing an SSH connection as well as for the actual Limesurvey database.</li> <li>Staging database related variables: The staging database is where the raw and intermediary data are stored. You can provide values for a MySQL DB / MariaDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"mysql+pymysql\"</code>) or a PostgresDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\"</code>). Other databases are currently not supported.</li> <li>Reporting database related variables:  The reporting database is where the final (i.e., reporting) data is stored. You can provide values for a MySQL DB / MariaDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"mysql+pymysql\"</code>) or a PostgresDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\"</code>). Other databases are currently not supported. It is possible to use the same database as staging and reporting database.</li> </ul> <pre><code># Variables required to connect with the Limesurvey Database\nLIMESURVEY_USE_SSH=True # set to False if ssh is not used\nLIMESURVEY_SSH_PORT=\"&lt;LIMESURVEY_SSH_PORT_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_HOST=\"&lt;LIMESURVEY_SSH_HOST_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_USERNAME=\"&lt;LIMESURVEY_SSH_USER_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_PASSWORD=\"&lt;LIMESURVEY_SSH_PW_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_DB_NAME=\"&lt;NAME OF THE LIMESURVEY DB&gt;\"\nLIMESURVEY_DB_PORT=\"&lt;PORT OF THE LIMESURVEY DB&gt;\"\nLIMESURVEY_DB_USERNAME=\"&lt;LIMESURVEY DB SQL USER&gt;\"\nLIMESURVEY_DB_PASSWORD=\"LIMESURVEY DB SQL PASSWORD\"\nLIMESURVEY_DB_HOST=\"127.0.0.1\" # always 127.0.0.1 if SSH is used, or actual host if SSH is not used\n\n# Variables for connecting with the database where you want raw and intermediary data to be stored\nSTAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\" # \"postgresql\" for a postgres DB or \"mysql+pymysql\" if Staging DB is a MYSQL DB (e.g., MariaDB)\nSTAGING_DB_USERNAME=\"&lt;SQL USER OF THE STAGING DB\"\nSTAGING_DB_PASSWORD=\"&lt;PASSWORD OF THE STAGING DB SQL USER&gt;\"\nSTAGING_DB_NAME=\"&lt;NAME OF THE STAGING DB&gt;\"\nSTAGING_DB_HOST=\"&lt;HOST OF THE STAGING DB&gt;\"\nSTAGING_DB_PORT=\"&lt;PORT OF THE STAGING DB&gt;\"\n\n# Variables for connecting with the database where you want the final (i.e., reporting) data to be stored\n# Can be the same database as the staging database\nREPORTING_DB_SQLALCHEMY_DRIVER=\"postgresql\" # \"postgresql\" for a postgres DB or \"mysql+pymysql\" if reporting DB is a MYSQL DB (e.g., MariaDB)\nREPORTING_DB_PASSWORD=\"&lt;PW OF THE REPORTING DB SQL USER&gt;\"\nREPORTING_DB_NAME=\"&lt;NAME OF THE REPORTING DB&gt;\"\nREPORTING_DB_HOST=\"&lt;HOST OF THE REPORTING DB&gt;\"\nREPORTING_DB_PORT=\"&lt;PORT OF THE REPORTING DB&gt;\"\nREPORTING_DB_USERNAME=\"&lt;SQL USER OF THE REPORTING DB&gt;\"\n</code></pre> <p>Next, you can start adding an ETL pipeline configuration as described in the User Guide section.</p> <p>Once you created a pipeline configuration, you can run it from your terminal by entering the following command:</p> <p><code>poetry run python -m limesurvey_etl --config_file &lt;PATH/TO/YOUR/config.yaml --steps all</code></p> <ul> <li>The <code>--config_file</code> flag specifies the path to the configuration <code>.yaml</code>-file to be used for the pipeline</li> <li>The <code>--steps</code> flag specifies which step(s) of the ETL pipeline should be executed. Choices:<ul> <li><code>extract</code>: Only the extract step is executed.</li> <li><code>transform</code>: Only the transform step is executed.</li> <li><code>load</code>: Only the load step is executed.</li> <li><code>all</code>: All steps (i.e., extract, transform, and load) are executed.</li> </ul> </li> </ul>"},{"location":"installation/#2-orchestrating-etl-pipelines-with-airflow","title":"2. Orchestrating ETL pipelines with Airflow","text":"<p>This project supports Airflow to orchestrate ETL pipelines.</p>"},{"location":"installation/#setting-necessary-environment-variables","title":"Setting necessary environment variables","text":"<p>In order to run Airflow and allow the ETL pipelines to communicate with the source and target databases, you must set a number of environment variables.</p> <p>Create a <code>.env</code> file and set the Airflow UID by running the following command from the project's root directory:</p> <pre><code>echo -e \"AIRFLOW_UID=$(id -u)\" &gt;&gt; .env\n</code></pre> <p>Open the <code>.env</code> file you just created in your IDE or text editor and append the following four blocks of environment variables to it:</p> <ul> <li>Airflow related variables: You can choose arbitrary values here. These are required for logging into the Airflow UI and the Airflow DB (advanced users), which contains Airflow related metadata.</li> <li>Limesurvey DB related variables: These variables are required for the platform to connect with the Limesurvey database. You must provide the variables for establishing an SSH connection as well as for the actual Limesurvey database.</li> <li>Staging database related variables: The staging database is where the raw and intermediary data are stored. You can provide values for a MySQL DB / MariaDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"mysql+pymysql\"</code>) or a PostgresDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\"</code>). Other databases are currently not supported. Note: If the database is running on the same machine as airflow, then set <code>STAGING_DB_HOST=host.docker.internal</code>!</li> <li>Reporting database related variables:  The reporting database is where the final (i.e., reporting) data is stored. You can provide values for a MySQL DB / MariaDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"mysql+pymysql\"</code>) or a PostgresDB (set <code>STAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\"</code>). Other databases are currently not supported. Note: If the database is running on the same machine as airflow, then set <code>REPORTING_DB_HOST=host.docker.internal</code>! It is possible to use the same database as staging and reporting database.</li> </ul> <pre><code># Variables required to connect with the Limesurvey Database\nLIMESURVEY_SSH_PORT=\"&lt;LIMESURVEY_SSH_PORT_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_HOST=\"&lt;LIMESURVEY_SSH_HOST_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_USERNAME=\"&lt;LIMESURVEY_SSH_USER_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_SSH_PASSWORD=\"&lt;LIMESURVEY_SSH_PW_IF_SSH_IS_USED_ELSE_DELETE_VARIABLE&gt;\"\nLIMESURVEY_DB_NAME=\"&lt;NAME OF THE LIMESURVEY DB&gt;\"\nLIMESURVEY_DB_PORT=\"&lt;PORT OF THE LIMESURVEY DB&gt;\"\nLIMESURVEY_DB_USERNAME=\"&lt;LIMESURVEY DB SQL USER&gt;\"\nLIMESURVEY_DB_PASSWORD=\"LIMESURVEY DB SQL PASSWORD\"\nLIMESURVEY_DB_HOST=\"127.0.0.1\" # or actual host if SSH is not used\n\n# Variables for connecting with the database where you want raw and intermediary data to be stored\nSTAGING_DB_SQLALCHEMY_DRIVER=\"postgresql\" # \"postgresql\" for a postgres DB or \"mysql+pymysql\" if Staging DB is a MYSQL DB (e.g., MariaDB)\nSTAGING_DB_USERNAME=\"&lt;SQL USER OF THE STAGING DB\"\nSTAGING_DB_PASSWORD=\"&lt;PASSWORD OF THE STAGING DB SQL USER&gt;\"\nSTAGING_DB_NAME=\"&lt;NAME OF THE STAGING DB&gt;\"\nSTAGING_DB_HOST=\"&lt;HOST OF THE STAGING DB&gt;\" # host.docker.internal for usage with ariflow if DB is running on same machine as airflow\nSTAGING_DB_PORT=\"&lt;PORT OF THE STAGING DB&gt;\"\n\n# Variables for connecting with the database where you want the final (i.e., reporting) data to be stored\n# Can be the same database as the staging database\nREPORTING_DB_SQLALCHEMY_DRIVER=\"postgresql\" # \"postgresql\" for a postgres DB or \"mysql+pymysql\" if reporting DB is a MYSQL DB (e.g., MariaDB)\nREPORTING_DB_PASSWORD=\"&lt;PW OF THE REPORTING DB SQL USER&gt;\"\nREPORTING_DB_NAME=\"&lt;NAME OF THE REPORTING DB&gt;\"\nREPORTING_DB_HOST=\"&lt;HOST OF THE REPORTING DB&gt;\" # host.docker.internal for usage with ariflow if DB is running on same machine as airflow\nREPORTING_DB_PORT=\"&lt;PORT OF THE REPORTING DB&gt;\"\nREPORTING_DB_USERNAME=\"&lt;SQL USER OF THE REPORTING DB&gt;\"\n</code></pre>"},{"location":"installation/#setup-airflow","title":"Setup Airflow","text":"<p>To start Airflow, make sure the Docker daemon is running (e.g., by starting docker desktop or starting the docker service) and run <pre><code>docker compose up\n</code></pre> This may take a while. Once the setup is complete, you should repeatedly see a message similar to the following in your terminal: <pre><code>limesurvey-etl-airflow-webserver-1  | 127.0.0.1 - - [19/Oct/2023:14:16:17 +0000] \"GET /health HTTP/1.1\" 200 141 \"-\" \"curl/7.74.0\"\n</code></pre></p> <p>The Airflow Webserver is now running and the Airflow UI can be accessed through a regular browser by entering the following URL: <code>localhost:8080</code>. Use the credentials defined in your <code>.env</code> file (<code>_AIRFLOW_WWW_USER_USERNAME</code> and <code>_AIRFLOW_WWW_USER_PASSWORD</code>) to login. You can add additional users via the \"Security\" tab at the top of the Airflow UI after successful login. Airflow DAGs will run depending on their schedules as long as Airflow is running. Press <code>control+c</code> or <code>strg+c</code> to stop the process, depending on your operating system.</p>"},{"location":"installation/#adding-etl-pipelines","title":"Adding ETL pipelines","text":"<p>Once Airflow is up and running, you can start adding your ETL pipelines as described in the User Guide.</p>"},{"location":"installation/#clean-up","title":"Clean up","text":"<p>To clean up the environment, run <pre><code>docker compose down --volumes --rmi all\n</code></pre> Note: This will not delete any Limesurvey data or data generated during an ETL pipeline run from your target databases, but only remove the Airflow environment.# Partner organization</p> <p>This project was conducted in collaboration with the Vielfalt entscheidet project of Citizens For Europe gUG.</p>"},{"location":"user-how-to/creating-dags/","title":"Creation and configuration of new ETL pipelines in Airflow","text":""},{"location":"user-how-to/creating-dags/#step-1-creating-an-etl-configuration","title":"Step 1: Creating an ETL configuration","text":"<ul> <li>add a new <code>.yaml</code> file and give it a descriptive name (e.g., <code>my_etl_pipeline.yaml</code>). If you want to run the pipeline with Airflow, you must place the <code>.yaml</code> file inside <code>airflow/dags/configs</code></li> <li> <p>add your desired ETL-steps (i.e., extract, transform, load) to the configuration file (see the ETL Configs How To for a detailed description of all available configuration options), for example:</p> <p><pre><code>extract:\n  extract_type: limesurvey_extract\n  tables:\n    - lime_questions\n\ntransformation_pipelines:\n  - table_name: question_items\n    staging_schema: staging\n    source_data:\n      source_tables:\n        - table_name: lime_questions\n          columns:\n            - qid AS question_item_id\n            - title AS question_item_title\n            - gid AS question_group_id\n            - type AS type_major\n            - type AS type_minor\n    transform_steps:\n      - transform_type: join_with_csv_mapping\n        # if using with airflow, place any mappings inside airflow/include/mappings/ and use the /opt/ prefix when specifying the path!\n        mapping_path: /opt/airflow/include/mappings/Mapping_LS-QuestionTypesMajor.csv\n        mapping_delimiter: ;\n        keep_columns:\n          - \"Fragetyp Major (CFE)\"\n        left_on: type_major\n        right_on: \"Fragetyp-ID (LS)\"\n        how: left\n      - transform_type: rename_columns\n        colname_to_colname: \"Fragetyp Major (CFE)\": type_major\n\nload:\n  load_type: reporting_db_load\n  staging_schema: staging\n  target_schema: reporting\n  tables:\n      - question_items\n</code></pre> In this example, the table <code>lime_questions</code> is extracted from Limesurvey and stored in the <code>raw</code> schema of the staging database based on the <code>.env</code> file (if you do not have an <code>.env</code> file yet, follow the procedure desscribed here).</p> <p>Next, a transformation pipeline is defined that transforms data based on <code>raw.lime_questions</code> (i.e., the extracted table) and stores it in a table <code>question_items</code> in the <code>staging</code> schema of the staging database. Note: if the table you want to store the data in does not yet exists and you want to take control over the column types (e.g., setting primary keys and column types), you can define that using the <code>columns</code> parameter. See the ETL Configs How To for details on how to configure it. The transformation pipeline consists of two transformation steps: first, the data is joined with a mapping <code>.csv</code>-file stored in the <code>airflow/include/mappings</code> directory (note: to be available, mapping files must be placed inside the <code>airflow/include</code> directory and the <code>/opt/</code> prefix is always required when specifying the path in the configuration file). Second, the column \"Fragetyp Major (CFE)\" is renamed to \"type_major\".</p> <p>Finally, the load step loads the <code>question_items</code> table to the <code>reporting</code> schema of the database.</p> </li> <li> <p>you can choose from a variety of extractors, transformers, and loaders that can be combined in any way</p> </li> <li>detailed documentation on the types of extractors, transformers, and loaders as well as their parameters can be found here</li> </ul>"},{"location":"user-how-to/creating-dags/#step-2-creating-a-dag","title":"Step 2: Creating a DAG","text":"<p>If you want to run your pipeline with Airflow, the next step is to create a DAG file. A DAG (Directed Acyclic Graph) is a pipeline that is executed by Airflow. To run your ETL job in Airflow, you must add a DAG-file.</p> <ul> <li>Add a file named <code>&lt;YOUR_PIPELINE_NAME&gt;_dag.py</code> to the <code>airflow/dags</code> directory</li> <li>Copy the following code and replace the placeholders with your values (Note: in a future version, this procedure will be simplified by allowing to create DAGs from configuration files):   <pre><code>from datetime import datetime, timedelta\nfrom pathlib import Path\n\nfrom airflow.contrib.hooks.ssh_hook import SSHHook\nfrom airflow.contrib.operators.ssh_operator import SSHOperator\nfrom airflow.models import Variable\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.utils.dates import days_ago\nfrom airflow.utils.task_group import TaskGroup\n\nfrom airflow import DAG\nfrom limesurvey_etl.etl_pipeline import Pipeline\n\n# REPLACE THE FOLLOWING VALUES\n\n# replace with the name of your etl configuration file\nCONFIG_FILE_NAME = \"&lt;YOUR_ETL_CONFIG_FILE&gt;.yaml\"\n# give your DAG a unique name (it will be used to be displayed in the Airflow UI)\nDAG_ID = \"&lt;YOUR_DAG_NAME&gt;\"\n# set the schedule defining when your DAG should run using a crontab or None\n# e.g., \"0 0 * * *\" for once a day at 12:00 AM\nDAG_SCHEDULE = \"YOUR_DAG_SCHEDULE\"\n\n# OPTIONALLY MODIFY THE AIRFLOW DEFAULT_ARGS (advanced users)\nDEFAULT_ARGS = {\n    \"start_date\": days_ago(1),\n    \"retries\": 1,\n    \"retry_delay\": timedelta(seconds=30),\n    \"execution_timeout\": timedelta(minutes=5),\n}\n\nPIPELINE = Pipeline.get_pipeline(Path(f\"/opt/airflow/dags/configs/{CONFIG_FILE_NAME}\"))\n\nwith DAG(\n    dag_id=DAG_ID,\n    default_args=DEFAULT_ARGS,\n    catchup=False,\n    max_active_runs=1,\n    schedule=DAG_SCHEDULE,\n) as dag:\n    extract_limesurvey_data = PythonOperator(\n        task_id=\"extract\", python_callable=PIPELINE.run_extract\n    )\n\n    with TaskGroup(group_id=\"transform\") as transform:\n        task_list = []\n        for transformation_pipeline in PIPELINE.transformation_pipelines:\n            task_id = transformation_pipeline[\"table_name\"]\n            task_list.append(\n                PythonOperator(\n                    task_id=task_id,\n                    python_callable=PIPELINE.run_transform,\n                    op_kwargs={\"table_name\": task_id},\n                )\n            )\n            for pos, _ in enumerate(task_list[:-1]):\n                task_list[pos] &gt;&gt; task_list[pos + 1]\n\n    load_data = PythonOperator(task_id=\"load\", python_callable=PIPELINE.run_load)\n\n\nextract_limesurvey_data &gt;&gt; transform &gt;&gt; load_data\n</code></pre></li> </ul>"},{"location":"user-how-to/creating-dags/#step-3-monitoring-running-a-dag","title":"Step 3: Monitoring / running a DAG","text":"<p>Save the changes you made. You should now be able to see the DAG with your <code>DAG_ID</code> in the Airflow UI. You can click on the DAG to open Airflow's grid view and monitor the DAG runs.</p>"},{"location":"user-how-to/etl-config-how-to/","title":"ETL Configs How To","text":"<p>Limesurvey ETL pipeline configurations can consist of a variety of extractors, transformers, and loaders. The following documentation provides a detailed description of how to configure them.</p>"},{"location":"user-how-to/etl-config-how-to/#how-to-write-custom-etl-configuration-yaml-files","title":"How to write custom ETL configuration yaml-files","text":"<p>The ETL configuration <code>-yaml</code>-file must define three main steps which are explained in detail in the below sections: extract, transform, and load. The basic structure of the yaml-file should look as follows:</p> <pre><code>extract:\n    &lt;EXTRACT TYPE CONFIGURATION&gt;\n\ntransformation_pipelines:\n    - &lt;TRANSFORMATION_PIPELINE_1&gt;\n    - &lt;TRANSFORMATION_PIPELINE_2&gt;\n    ...\n    - &lt;TRANSFORMATION_PIPELINE_3&gt;\n\nload:\n    &lt;LOAD TYPE CONFIGURATION&gt;\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#step-1-define-the-extract-step","title":"Step 1: Define the extract step","text":"<p>The extract step defines what data is extracted from limesurvey. Currently, this project only supports extracting data from the Limesurvey DB. Support for the Limesurvey API might be added in the future. Add an extract step of type <code>limesurvey_extract</code> (see example) uder the <code>extract</code> field of your configuration file:</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.extract_config.limesurvey.LimesurveyExtractConfig","title":"<code>LimesurveyExtractConfig</code>","text":"<p>Configuration for extracting data from LimeSurvey.</p> <p>Attributes:</p> Name Type Description <code>extract_type</code> <code>Literal['limesurvey_extract']</code> <p>Type of extraction, should be \"limesurvey_extract\".</p> <code>tables</code> <code>list[str]</code> <p>List of tables to be extracted.</p> <code>staging_schema</code> <code>str</code> <p>Name of the schema to store raw data in the staging area. Default is \"raw\".</p> <code>use_ssh</code> <code>bool</code> <p>Whether to ssh into a server before running extract step. Default is True.</p>"},{"location":"user-how-to/etl-config-how-to/#example","title":"Example","text":"<pre><code>extract_type: limesurvey_extract\n  tables:\n    - lime_questions\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#step-2-define-the-transform-steps","title":"Step 2: Define the transform steps","text":"<p>Transform steps define what sequence of transformations will be applied to the extracted data. You can chain multiple transformation pipelines in one ETL pipeline so that multiple tables will be created in the <code>staging</code> schema of your target database.</p> <p>You can define a transformation pipeline by configuring the following parameters:</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.transformation_pipeline.TransformationPipelineConfig","title":"<code>TransformationPipelineConfig</code>","text":"<p>Configuration for a transformation pipeline.</p> <p>Attributes:</p> Name Type Description <code>table_name</code> <code>str</code> <p>Name of the table to be created in the staging area.</p> <code>transform_steps</code> <code>Sequence[Union[AddColumnsConfig, FillNullValuesConfig, FilterDataConfig, JoinWithCSVMappingConfig, RenameColumnsConfig, ReplaceValuesConfig, AddComputedColumnConfig]]</code> <p>Sequence of transform step configs. If None, only an empty table is created.</p> <code>staging_schema</code> <code>str</code> <p>Name of the schema to store data in the staging area.</p> <code>columns</code> <code>list[Column]</code> <p>List of column definitions relevant in case the staging table does not yet exists in the database. See Column class for relevant items for the config dicts. If None (default), table is created directly from pandas df.</p> <code>source_data</code> <code>SelectSourceDataConfig</code> <p>Source data from the extracted data the transformations should be applied to.</p>"},{"location":"user-how-to/etl-config-how-to/#_1","title":"ETL Configs How To","text":"<p>The <code>columns</code>, <code>data</code>, and <code>transform_steps</code> fields require additional configurations as described below:</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.transformation_pipeline.Column","title":"<code>Column</code>","text":"<p>Represents a column in a database table.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the column.</p> <code>type</code> <code>str</code> <p>Valid sqlalchemy data type, such as VARCHAR(255) or INTEGER.</p> <code>primary_key</code> <code>bool</code> <p>Whether or not the column is a primary key column in the reporting table.</p> <code>nullable</code> <code>bool</code> <p>Whether or not the column can be Null.</p> <code>foreign_key</code> <code>Optional[str]</code> <p>The foreign key associated with the column, if any.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.select_source_data.SelectSourceDataConfig","title":"<code>SelectSourceDataConfig</code>","text":"<p>Configuration for selecting a subset of data from source tables.</p> <p>Attributes:</p> Name Type Description <code>source_schema</code> <code>str</code> <p>Name of the database schema the source data is stored in. Defaults to 'raw'</p> <code>right_table_source_schema</code> <code>Optional[str]</code> <p>Schema name of the right table if different from left table source schema. If none, source_schema is used for right table as well.</p> <code>source_tables</code> <code>list[SourceTable]</code> <p>List of source tables with their columns to be selected.</p> <code>join</code> <code>Join</code> <p>Join configuration based on the Join class.</p> <code>filter</code> <code>str</code> <p>Optional SQL filter clause.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.select_source_data.SourceTable","title":"<code>SourceTable</code>","text":"<p>Represents a source table.</p> <p>Attributes:</p> Name Type Description <code>table_name</code> <code>str</code> <p>Name of the source table.</p> <code>columns</code> <code>list[str]</code> <p>List of column names in the source table.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.select_source_data.Join","title":"<code>Join</code>","text":"<p>Represents a join operation.</p> <p>Attributes:</p> Name Type Description <code>type</code> <code>str</code> <p>Type of join operation.</p> <code>left_table</code> <code>str</code> <p>Name of the join's left table.</p> <code>right_table</code> <code>str</code> <p>Name of the join's right table.</p> <code>left_on</code> <code>Optional[str]</code> <p>Column name for joining on the left table.</p> <code>right_on</code> <code>Optional[str]</code> <p>Column name for joining on the right table.</p>"},{"location":"user-how-to/etl-config-how-to/#example_1","title":"Example","text":"<pre><code>- table_name: subquestions\n  staging_schema: staging\n  columns:\n    - name: subquestion_id\n      type: INTEGER\n      primary_key: True\n      nullable: False\n    - name: question_item_id\n      type: INTEGER\n      nullable: False\n      foreign_key: staging.question_items.question_item_id\n    - name: subquestion_item_title\n      type: VARCHAR(50)\n    - name: question_item_title\n      type: VARCHAR(50)\n  source_data:\n    source_tables:\n      - table_name: lime_questions\n        columns:\n          - qid AS subquestion_id\n          - title AS subquestion_item_title\n      - table_name: lime_questions\n        columns:\n          - qid AS question_item_id\n          - title AS question_item_title\n    source_schema: raw\n    join:\n      type: JOIN\n      left_table: lime_questions\n      right_table: lime_questions\n      left_on: parent_qid\n      right_on: qid\n    filter: \"WHERE left_table.parent_qid != 0\"\n  transform_steps:\n    - transform_type: add_computed_column\n      column_name: subquestion_item_title\n      input_columns:\n        - question_item_title\n        - subquestion_item_title\n      operator:\n        name: concat\n        separator: \"_\"\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#full-list-of-transform-steps","title":"Full list of transform steps","text":"<p>The <code>transform_steps</code> field is a list of transformations that should be applied to the data. There are a number of different transform steps available as documented below.</p> <ul> <li><code>add_columns</code></li> <li><code>melt_data</code></li> <li><code>add_computed_column</code></li> <li><code>fill_null_values</code></li> <li><code>filter_data</code></li> <li><code>join_with_csv_mapping</code></li> <li><code>rename_columns</code></li> <li><code>replace_values</code></li> <li><code>cast_data_type</code></li> </ul> <p>Note: transformations are applied in order.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_columns.AddColumnsConfig","title":"<code>AddColumnsConfig</code>","text":"<p>Configuration for adding columns to a table.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['add_columns']</code> <p>Type of transformation, should be \"add_columns\".</p> <code>column_names</code> <code>Union[str, list[str]]</code> <p>Name of the columns to be added.</p> <code>default_value</code> <code>Union[str, int, bool, float]</code> <p>If set, columns will contain this value. Else, columns will be null.</p>"},{"location":"user-how-to/etl-config-how-to/#example_2","title":"Example","text":"<pre><code>- transform_type: add_columns\n  column_names:\n    - added_column_1\n    - added_column_2\n  default_value: 1000\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.melt_data.MeltDataConfig","title":"<code>MeltDataConfig</code>","text":"<p>Configuration for unpivoting (melting) a DataFrame from wide to long format.</p> <p>Massage data into a format where one or more columns are identifier variables (id_vars), while all other columns, considered measured variables (value_vars), are 'unpivoted' to the row axis, leaving just two non-identifier columns, 'variable' and 'value.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['melt_data']</code> <p>The type of data transformation.</p> <code>id_vars</code> <code>Optional[list]</code> <p>Column(s) to use as identifier variables. Default is None.</p> <code>value_vars</code> <code>Optional[list]</code> <p>Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars.</p> <code>var_name</code> <code>str</code> <p>Name to use for the 'variable' column.</p> <code>value_name</code> <code>str</code> <p>Name to use for the 'value' column.</p>"},{"location":"user-how-to/etl-config-how-to/#example_3","title":"Example","text":"<pre><code>- trransform_type: melt_data\n  id_vars:\n    - id\n  value_vars:\n    - type\n</code></pre> <p>The <code>add_computed_columns</code> configuration requires to define an operation in the <code>operator</code> field. The following operators are available:</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.AddComputedColumnConfig","title":"<code>AddComputedColumnConfig</code>","text":"<p>Configuration for adding a computed column.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['add_computed_column']</code> <p>Type of transformation, should be \"add_computed_column\".</p> <code>column_name</code> <code>Union[str, list[str]]</code> <p>Name(s) of the new column(s).</p> <code>input_columns</code> <code>Union[str, list[str]]</code> <p>Column(s) that should be used to compute the new column(s).</p> <code>operator</code> <code>Union[SumOperator, ProductOperator, DifferenceOperator, ConcatOperator, SplitOperator, ExtractOperator]</code> <p>Operation used to compute the new column.</p> <code>drop_input_columns</code> <code>Union[Literal['all'], list[str]]</code> <p>Input columns to be dropped from data frame after computation.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.SumOperator","title":"<code>SumOperator</code>","text":"<p>Model representing a 'sum' operator. Computes the sum of values.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['sum']</code> <p>The name of the operator, which should be \"sum\".</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.ProductOperator","title":"<code>ProductOperator</code>","text":"<p>Model representing a 'product' operator. Computes the product of values.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['product']</code> <p>The name of the operator, which should be \"product\".</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.DifferenceOperator","title":"<code>DifferenceOperator</code>","text":"<p>Model representing a 'difference' operator. Subtracts the values.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['difference']</code> <p>The name of the operator, which should be \"difference\".</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.ConcatOperator","title":"<code>ConcatOperator</code>","text":"<p>Model representing a 'concat' operator. Concatenates the values using a separator.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['concat']</code> <p>The name of the operator, which should be \"concat\".</p> <code>separator</code> <code>str</code> <p>Separator used to separate strings.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.SplitOperator","title":"<code>SplitOperator</code>","text":"<p>Model representing a 'split' operator. Splites a value based on a delimiter.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['split']</code> <p>The name of the operator, which should be \"split\".</p> <code>delimiter</code> <code>str</code> <p>Delimiter used to split strings.</p> <code>expand</code> <code>bool</code> <p>Whether or not to expand the split values when using the 'split' operator.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.add_computed_column.ExtractOperator","title":"<code>ExtractOperator</code>","text":"<p>Model representing an 'extract' operator. Extracts a match from a string based on a regular expression.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>Literal['extract']</code> <p>The name of the operator, which should be \"extract\".</p> <code>regex</code> <code>str</code> <p>Regular expression used to extract from string.</p> <code>expand</code> <code>bool</code> <p>Whether or not to expand the values if multiple matches found.</p>"},{"location":"user-how-to/etl-config-how-to/#example_4","title":"Example","text":"<pre><code>- transform_type: add_computed_column\n  column_name: subquestion_item_title\n  input_columns:\n    - question_item_title\n    - subquestion_item_title\n  operator:\n    name: concat\n    separator: \"_\"\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.fill_null_values.FillNullValuesConfig","title":"<code>FillNullValuesConfig</code>","text":"<p>Configuration for filling null values in a column.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['fill_null_values']</code> <p>Type of transformation, should be \"fill_null_values\".</p> <code>column_name</code> <code>str</code> <p>Name of the column in which Null values should be replaced.</p> <code>value</code> <code>Union[str, int, float, bool]</code> <p>Value to replace Null values with.</p> <code>method</code> <code>Union[Literal['backfill'], Literal['ffill']]</code> <p>Method to use for filling Null values in Column. 'ffill' -&gt; propagate last valid observation forward to next valid. 'backfill' -&gt; use next valid observation to fill gap.</p>"},{"location":"user-how-to/etl-config-how-to/#example_5","title":"Example","text":"<pre><code>- transform_type: fill_null_values\n  column_name: lang\n  value: \"99\"\n</code></pre> <p>The <code>filter_data</code> transform step requires you to set one or multiple filter <code>conditions</code> as follows:</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.filter_data.FilterDataConfig","title":"<code>FilterDataConfig</code>","text":"<p>Configuration for filtering data.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['filter_data']</code> <p>Type of transformation, should be \"filter_data\".</p> <code>conditions</code> <code>list[FilterCondition]</code> <p>List of FilterCondition objects.</p> <code>logical_operator</code> <code>Union[Literal['AND'], Literal['OR']]</code> <p>Logical operator to be applied if multiple conditions should be applied.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.filter_data.FilterCondition","title":"<code>FilterCondition</code>","text":"<p>Represents a filtering condition.</p> <p>Attributes:</p> Name Type Description <code>column</code> <code>str</code> <p>The column name to apply the condition on.</p> <code>value</code> <code>str</code> <p>The value to compare against.</p> <code>operator</code> <code>Union[Literal['=='], Literal['!='], Literal['&gt;'], Literal['&lt;'], Literal['&gt;='], Literal['&lt;='], Literal['contains'], Literal['not_contains']]</code> <p>The comparison operator.</p>"},{"location":"user-how-to/etl-config-how-to/#example_6","title":"Example","text":"<pre><code>- transform_type: filter_data\n  conditions:\n    - column: \"age\"\n      value: \"30\"\n      operator: \"&gt;=\"\n    - column: \"gender\"\n      value: \"female\"\n      operator: \"==\"\n  logical_operator: AND\n</code></pre> <p>You can also use the <code>filter_data</code> transform step multiple times to perform more sophisticated filter operations.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.join_with_csv_mapping.JoinWithCSVMappingConfig","title":"<code>JoinWithCSVMappingConfig</code>","text":"<p>Configuration for joining data with a CSV mapping.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['join_with_csv_mapping']</code> <p>Type of transformation, should be \"join_with_csv_mapping\".</p> <code>mapping_path</code> <code>str</code> <p>Path to the CSV mapping file.</p> <code>mapping_delimiter</code> <code>str</code> <p>Delimiter used in the CSV mapping file. Default is \",\".</p> <code>keep_columns</code> <code>list[str]</code> <p>List of columns to keep from mapping table. If None, all columns are kept.</p> <code>how</code> <code>Union[Literal['left'], Literal['right'], Literal['outer'], Literal['inner'], Literal['cross']]</code> <p>Type of join to be performed. Default is \"inner\".</p> <code>on</code> <code>Union[str, list[str]]</code> <p>Column names to join on. If None, then both left_on and right_on must be specified.</p> <code>left_on</code> <code>Union[str, list[str]]</code> <p>Column names to join on in the original data frame.</p> <code>right_on</code> <code>Union[str, list[str]]</code> <p>Column names to join on in the mapping data frame.</p>"},{"location":"user-how-to/etl-config-how-to/#example_7","title":"Example","text":"<pre><code>- transform_type: join_with_csv_mapping\n  mapping_path: \"/opt/airflow/include/mappings/mapping_question_items.csv\"\n  mapping_delimiter: \",\"\n  keep_columns:\n    - label_major\n    - label_minor\n  left_on: question_item_title\n  right_on: question_item_id\n  how: left\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.rename_columns.RenameColumnsConfig","title":"<code>RenameColumnsConfig</code>","text":"<p>Configuration for renaming columns.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['rename_columns']</code> <p>Type of transformation, should be \"rename_columns\".</p> <code>colname_to_colname</code> <code>dict[str, str]</code> <p>Dictionary containing the original column names as keys and the new column names as values.</p>"},{"location":"user-how-to/etl-config-how-to/#example_8","title":"Example","text":"<pre><code>- transform_type: rename_columns\n  colname_to_colname:\n    \"Fragetyp Major (CFE)\": type_major\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.replace_values.ReplaceValuesConfig","title":"<code>ReplaceValuesConfig</code>","text":"<p>Configuration for replacing values in columns.</p> <p>Attributes:</p> Name Type Description <code>transform_type</code> <code>Literal['replace_values']</code> <p>Type of transformation, should be \"replace_values\".</p> <code>replacement_values</code> <code>dict[str, dict[Union[str, int, float, bool], Union[str, int, float, bool]]]</code> <p>Dictionary containing the replacement values. Dictionary keys indicate values to be replaced by corresponding dictionary values.</p>"},{"location":"user-how-to/etl-config-how-to/#example_9","title":"Example","text":"<pre><code>- transform_type: replace_values\n  column_name: age\n  replacement_values:\n    999: 30\n    \"Unknown\": \"Other\"\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.transform_config.cast_data_type.CastDataTypeConfig","title":"<code>CastDataTypeConfig</code>","text":""},{"location":"user-how-to/etl-config-how-to/#example_10","title":"Example","text":"<pre><code>- transform_type: cast_data_type\n  column_names:\n    - question_id\n    - survey_id\n  target_data_type: int\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#step-3-define-the-load-step","title":"Step 3: Define the Load Step","text":"<p>The load step defines how the data from the <code>staging</code> schema is loaded to the <code>reporting</code> schema of your target database.</p>"},{"location":"user-how-to/etl-config-how-to/#limesurvey_etl.config.load_config.reporting_db.ReportingDBLoadConfig","title":"<code>ReportingDBLoadConfig</code>","text":"<p>Configuration for loading data into the reporting database.</p> <p>Attributes:</p> Name Type Description <code>load_type</code> <code>Literal['reporting_db_load']</code> <p>Type of load, should be \"reporting_db_load\".</p> <code>tables</code> <code>list[str]</code> <p>Tables to be loaded to the target. Must be in the correct order if foreign keys are used.</p> <code>staging_schema</code> <code>str</code> <p>If applicable, the name of the schema in the staging area to load the data from.</p> <code>target_schema</code> <code>str</code> <p>Name of the schema to load the data to.</p>"},{"location":"user-how-to/etl-config-how-to/#example_11","title":"Example","text":"<pre><code>load:\n  load_type: reporting_db_load\n  staging_schema: staging\n  target_schema: reporting\n  tables:\n    - question_groups\n    - question_items\n    - subquestions\n</code></pre>"},{"location":"user-how-to/etl-config-how-to/#example-of-an-etl-configuration-file","title":"Example of an ETL configuration file","text":"<p>For a more complicated example, check out <code>airflow/dags/configs/cfe_limesurvey.yaml</code> in the repository.</p> <pre><code>extract:\n  extract_type: limesurvey_extract\n  tables:\n    - lime_questions\n\ntransformation_pipelines:\n  - table_name: question_items\n    staging_schema: staging\n    columns:\n      - name: question_item_id\n        type: INTEGER\n        primary_key: True\n        nullable: False\n      - name: question_item_title\n        type: VARCHAR(255)\n        nullable: True\n      - name: question_group_id\n        type: INTEGER\n        nullable: False\n        foreign_key: \"staging.question_groups.question_group_id\"\n      - name: type_major\n        type: VARCHAR(255)\n        nullable: True\n      - name: type_minor\n        type: VARCHAR(255)\n        nullable: True\n    source_data:\n      source_tables:\n        - table_name: lime_questions\n          columns:\n            - qid AS question_item_id\n            - title AS question_item_title\n            - gid AS question_group_id\n            - type AS type_major\n            - type AS type_minor\n      source_schema: raw\n    transform_steps:\n      - transform_type: join_with_csv_mapping\n        mapping_path: /opt/airflow/include/mappings/Mapping_LS-QuestionTypesMajor.csv\n        mapping_delimiter: ;\n        keep_columns:\n          - \"Fragetyp Major (CFE)\"\n        left_on: type_major\n        right_on: \"Fragetyp-ID (LS)\"\n        how: left\n      - transform_type: rename_columns\n        colname_to_colname: \"Fragetyp Major (CFE)\": type_major\n\nload:\n  load_type: reporting_db_load\n  staging_schema: staging\n  target_schema: reporting\n  tables:\n      - question_items\n</code></pre>"}]}